{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages and utilities\n",
    "import importlib\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from pickle import dump\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "# Update the matplotlib font configuration\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "# mlxtend\n",
    "import mlxtend\n",
    "from mlxtend.plotting import plot_pca_correlation_graph\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Crippen, Descriptors, Draw, rdMolDescriptors\n",
    "from rdkit.Chem.Draw import DrawingOptions, IPythonConsole\n",
    "from rdkit.ML.Cluster import Butina\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "from sklearn import linear_model, metrics, svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, DotProduct, Matern, RBF, WhiteKernel\n",
    "from sklearn.linear_model import LassoCV, LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# scikitplot library\n",
    "import scikitplot as skplt\n",
    "\n",
    "# seaborn\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ModuleNotFoundError:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns\n",
    "\n",
    "# own module\n",
    "from pythia import classification_metrics as cmetrics\n",
    "from pythia import fingerprints_generation as fp\n",
    "from pythia import molecules_and_structures as ms\n",
    "from pythia import plots as pltsk\n",
    "from pythia import scaling\n",
    "from pythia import workflow_functions as wf\n",
    "\n",
    "# utility\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(format='%(message)s')\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# Random seed setup\n",
    "random_seed = 10459\n",
    "np.random.seed = random_seed\n",
    "np.random.RandomState(random_seed)\n",
    "log.info(f\"Random seed fixed as {random_seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook apart from reproducing the results of the paper also attempts to show you how to use precalculated descriptors to train your model. In this case we used DFT descriptors, however you could use any type of descriptors you wish. It explains how to use PCA (although it is not used for the results of this paper) as well as how to get the most important features from LASSOCV. This is used in the results of the paper. Please follow step by step of feel free to skip steps you might not be interested in like PCA or LASSOCV interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load target data\n",
    "\n",
    "First we read the dataset from a csv file. A csv file is a comma separated file, where each row is a data point.\n",
    "pd.read_csv() is used to read the input csv file that contain the dataset. The dataset is stored in a dataframe, which is a data structure provided by the pandas library. A dataframe is a two-dimensional data structure, i.e. data is aligned in a tabular fashion in rows and columns. The dataframe can be indexed by column names.\n",
    "Here, we also reformat the column names to replace spaces with underscores and make the column names all lower case. This is done to make it easier to work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one csv with the DFT data and one with the targets (ddg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"DFTdata.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg = pd.read_csv(\"DDG.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg = ddg[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I make for you a folder to save all the models you create with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results_reg_DFT\", exist_ok=True)\n",
    "os.chdir(\"results_reg_DFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Example\n",
    "Here we show how you can perform PCA to reduce the features.\n",
    "\n",
    "If you do not know what PCA is or how it works please read\n",
    "https://en.wikipedia.org/wiki/Principal_component_analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "y = ddg # is not used\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_rescaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(data_rescaled)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,6)\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(0, 44, step=1) # change 44 according to number of features!!!\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 44, step=1)) #change here as well\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important descriptors are 12 according to the plot above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components = 0.95, whiten=True).fit(data_rescaled)\n",
    "X_pc = model.transform(data_rescaled)\n",
    "\n",
    "n_pcs= model.components_.shape[0]\n",
    "print(n_pcs)\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "# if your features have names, as they should, you can do the following. if not no need to remove from comments\n",
    "# Name all features in order as they appear in your csv eg.\n",
    "# initial_feature_names = ['HOMOsub', 'LUMOsub', 'C1', 'N1', 'Cindole', 'Dipolesub', 'L(N-C1)',\n",
    "#        'B1(N-C1)', 'B5(N-C1)', 'L(N-C3)', 'B1(N-C3)', 'B5(N-C3)', 'L(N-C12)',\n",
    "#        'B5(H-Cind+A2ole)', 'B5(N-C12)', 'L(H-Cindole)', 'B1(H-Cindole)',\n",
    "#        'B5(H-Cindole)', 'HOMO', 'LUMO', 'TOTAL NH', 'TOTAL CN', 'TOTAL N49-H',\n",
    "#        'Cl- Charge', 'TOTAL Hcharge', 'AVG H CHRG', 'DIPOLE', '49Cl NMR']\n",
    "# get the names\n",
    "# most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "# print(most_important_names)\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "# dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# If no names are included continue here\n",
    "dic = {'PC{}'.format(i+1): most_important[i] for i in range(n_pcs)}\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(dic.items())\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make dataframe only containing the PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [col for col in data.columns if col in df[1].values and data[col].all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = data[col_names]\n",
    "print(data_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call our classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add or remove regressors you might want (or don't) to explore, here we just offer some examples.\n",
    "\n",
    "kfold_reg_names = [\"LassoCV\",\"KNeighborsRegressor\", \"DecisionTreeRegressor\", \"SVR\",\n",
    "                   \"BayesianRegr\", \"GaussianProcessRegressor\", \"RandomForestRegressor\"]\n",
    "kfold_regressors = [\n",
    "    LassoCV(random_state=random_seed, cv=10,selection='random',max_iter=1000000),\n",
    "    KNeighborsRegressor(),\n",
    "    DecisionTreeRegressor(random_state=random_seed),\n",
    "    svm.SVR(), #SVR takes some time be patient if you are not, comment it out\n",
    "    linear_model.BayesianRidge(n_iter=100000),\n",
    "    GaussianProcessRegressor(),\n",
    "    RandomForestRegressor(random_state=random_seed)]# takes some time, you can remove some options for the parameters\n",
    "\n",
    "kfold_regressors_parameters = {\n",
    "    \"LassoCV\":{},\n",
    "    \"KNeighborsRegressor\": {\"n_neighbors\": [ent for ent in range(2, 10, 1)]},\n",
    "    \"DecisionTreeRegressor\": {\"max_depth\": [2, 3, 4, 5, 7, 10]},\n",
    "    \"SVR\": {\"kernel\":['linear', 'poly', 'rbf'], \"degree\":[2,3], \"gamma\":['auto','scale'], \"coef0\":[0,1], 'C':[100]},    \n",
    "    \"BayesianRegr\":{'alpha_1':[1e-06, 10], 'alpha_2': [1e-06,10],'lambda_1':[1e-06,10], 'lambda_2': [1e-06,10]},\n",
    "    \"GaussianProcessRegressor\": {},  \n",
    "    \"RandomForestRegressor\":{'n_estimators': [10,20],'criterion': ['friedman_mse', 'absolute_error', 'squared_error', 'poisson'],'max_depth': [3, 4, 5, 7, 10],\n",
    "    'max_features': ['auto','sqrt','log2'],'bootstrap': [True, False],'warm_start': [True, False]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we train the model based on the features that PCA selected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "wf.kfold_test_regressor_with_optimization(data_pca,ddg, kfold_regressors, kfold_regressors_parameters, scale=False, cv=5, n_repeats=10, rgs_names=kfold_reg_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_names = wf.directory_names(kfold_reg_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wf.build_data_from_directory_regr(directory_names[0], max_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wf.metrics_for_regression(directories=directory_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you continue please take a moment to look at your results! If you continue blindly, some of the results might be overwritten by the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSOCV Example\n",
    "\n",
    "Here I show you how to extract important descriptors from LASSO. LASSO uses the L1 regularization term that shrinks features to 0. If you dont know about LASSO read here https://en.wikipedia.org/wiki/Lasso_(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import again our data (DFT descriptors and DDG target values) as we have made changes on the firstly imported data. Remember we have changed directories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../DFTdata.csv\",header=None)\n",
    "ddg = pd.read_csv(\"../DDG.csv\",header=None)\n",
    "ddg = ddg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Coefficients,Feats = [],[]\n",
    "\n",
    "X = pd.read_csv(\"../DFTdata.csv\",header=None)\n",
    "Y = pd.read_csv(\"../DDG.csv\",header=None)\n",
    "Y=Y[0]\n",
    "\n",
    "X = scaling.minmaxscale(X)\n",
    "\n",
    "model=LassoCV(cv=10,selection='random',fit_intercept=True, max_iter=100000000).fit(X,Y)\n",
    "\n",
    "#Training set\n",
    "predicted=model.predict(X)\n",
    "\n",
    "print('Root Mean Squared Error Training Set:', np.sqrt(metrics.mean_squared_error(Y, predicted)))\n",
    "print('R2 Training Set:',r2_score(Y, predicted))\n",
    "\n",
    "\n",
    "#Coefficients\n",
    "log.info(\"\\n-----\\nCoefficients\\n-----\\n\")\n",
    "log.info(model.coef_)\n",
    "print(len(model.coef_))\n",
    "for i in range (len(model.coef_)):\n",
    "    Coefficients.append(model.coef_[i])\n",
    "    log.info(model.coef_[i])\n",
    "Feat1=np.nonzero(model.coef_)\n",
    "Feats.append(Feat1)\n",
    "log.info(Feat1)\n",
    "# X2=X[:,Feat1]\n",
    "# X2.shape\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now first we split 90%-10% (train-test)\n",
    "\n",
    "We need to evaluate the performace of our model on an external test set. We know which data points we are using for test set, for consistency reasons. However if you have much more data and you dont know which ones you want to use as a training set, or you don't want to write them manually you could do something like this:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df.drop('target', axis=1) y = df['target']\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "print(\"X_test:\") print(X_test)\n",
    "\n",
    "print(\"y_test:\") print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../DFTdata.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reactions = [21, 81, 11, 104, 44, 106, 54, 95, 115, 25, 57, 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.drop(labels =test_reactions, axis=0,inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame()\n",
    "test_data = data.iloc[test_reactions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg = pd.read_csv(\"../DDG.csv\", header = None)\n",
    "ddg = ddg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg_train = ddg.drop(labels =test_reactions, axis=0,inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg_test = ddg.iloc[test_reactions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Xtrain, Xtest, Ytrain, Ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = train_data\n",
    "Xtest = test_data\n",
    "Ytrain = ddg_train\n",
    "Ytest = ddg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add or remove regressors you might want (or don't) to explore, here we just offer some examples.\n",
    "\n",
    "kfold_reg_names = [\"LassoCV\",\"KNeighborsRegressor\", \"DecisionTreeRegressor\", \"SVR\",\n",
    "                   \"BayesianRegr\", \"GaussianProcessRegressor\", \"RandomForestRegressor\"]\n",
    "kfold_regressors = [\n",
    "    LassoCV(random_state=random_seed, cv=10,selection='random',max_iter=1000000),\n",
    "    KNeighborsRegressor(),\n",
    "    DecisionTreeRegressor(random_state=random_seed),\n",
    "    svm.SVR(), #SVR takes some time be patient if you are not, comment it out\n",
    "    linear_model.BayesianRidge(n_iter=100000),\n",
    "    GaussianProcessRegressor(),\n",
    "    RandomForestRegressor(random_state=random_seed)]# takes some time, you can remove some options for the parameters\n",
    "\n",
    "kfold_regressors_parameters = {\n",
    "    \"LassoCV\":{},\n",
    "    \"KNeighborsRegressor\": {\"n_neighbors\": [ent for ent in range(2, 10, 1)]},\n",
    "    \"DecisionTreeRegressor\": {\"max_depth\": [2, 3, 4, 5, 7, 10]},\n",
    "    \"SVR\": {\"kernel\":['linear', 'poly', 'rbf'], \"degree\":[2,3], \"gamma\":['auto','scale'], \"coef0\":[0,1], 'C':[100]},    \n",
    "    \"BayesianRegr\":{'alpha_1':[1e-06, 10], 'alpha_2': [1e-06,10],'lambda_1':[1e-06,10], 'lambda_2': [1e-06,10]},\n",
    "    \"GaussianProcessRegressor\": {},  \n",
    "    \"RandomForestRegressor\":{'n_estimators': [10,20],'criterion': ['friedman_mse', 'absolute_error', 'squared_error', 'poisson'],'max_depth': [3, 4, 5, 7, 10],\n",
    "    'max_features': ['auto','sqrt','log2'],'bootstrap': [True, False],'warm_start': [True, False]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "wf.split_test_regressors_with_optimization(Xtrain,Ytrain,Xtest,Ytest, kfold_regressors, kfold_regressors_parameters, scale=True, cv=5, n_repeats=10, rgs_names=kfold_reg_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythiachem",
   "language": "python",
   "name": "pythiachem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
