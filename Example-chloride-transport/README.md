# Example - chloride transport
In this example, we investigate the chloride transport activity of synthetic anion transporters. These are small molecule ligands that can bind anions and transport them across membranes operating via a mobile carrier mechanism, with therapeutic potential in ion channel-related diseases. Here, we use classification to identify whether a transporter can be considered active or inactive based on its $log(EC_{50})$ value. 

# Notebooks:
## Notebook: Dataset Analysis
This notebook provides a comprehensive exploration of the dataset through various analytical and visualization techniques. The goal is to gain insights into target distribution, chemical diversity, and the potential influence of binding motifs on target and class distributions. 

The dataset is imported as a .csv file containing the SMILES entries for the molecules under study, along with any other information. The standard analysis comprises visualizing 2D chemical structures, computing the Tanimoto similarity matrix to gauge structural diversity, investigating the distribution of the target variable and assessing class imbalances. Chemical space visualization is also supported, which uses techniques like PCA, t-SNE, and UMAP to gain insights into the underlying structure and relationships between the molecules in the dataset. It can help visualize molecular similarities and identify potential clusters or trends. This is especially useful when exploring the chemical space or assessing the diversity of a compound library.

Furthermore, it introduces an advanced analysis section, optional for users, focusing on recognized binding motifs for anion transporters. This section covers substructure matching against binding motifs, exploring overlap between motif-specific datasets, and assessing Tanimoto similarity within substructure sets. Additionally, it delves into the target and class distributions of each substructure dataset, shedding light on potential patterns and imbalances. This comprehensive notebook combines standard exploratory analyses with optional advanced analyses that delve into substructure-specific insights. It enables users to gain a deep understanding of the dataset's properties and the potential impact of binding motifs on various aspects of the data.

## Notebook: Classification-Mordred
In this Notebook, we harness the power of Mordred descriptors due to their ability to offer a diverse and comprehensive set of features encompassing topological, geometric, and electronic properties. We compute 1613 2D Mordred descriptors. After feature generation, data cleaning is performed to enhance data quality and relevance. Specifically, data cleaning involves (i) dropping descriptors (columns) containing more than 90\% of missing values (sometimes the Mordred calculator fails to generate descriptors for all molecules resulting in missing values) and (ii) columns with standard deviation less than 0.5 (as they may exhibit limited variability and thus contribute less valuable information to the analysis).

As not all descriptors will be relevant for the prediction of our target, an important step is to determine which descriptors correlate with the target value. Feature selection is conducted using correlation analysis. We suggest using Pearson correlation for continuous data and Spearman correlation for categorical data. In this example, Spearman correlation coefficients are computed to identify descriptors that correlate with the target value. Custom features are added. To address class imbalance, we generate synthetic data using SMOTENC, considering both categorical and non-categorical data. Once the synthetic points data frame is constructed, the classification algorithms are trained, and model evaluation is performed. 

For this task, we considered several classifiers, including Ada Boost, Extra Trees, Logistic Regression, Decision Tree, Gaussian Process and k-Nearest Neighbour. To evaluate model performance, we used multiple evaluation metrics, including (1) Accuracy, which measures the ratio of correctly classified instances to the total number of instances in the dataset, (2) Sensitivity (True Positive Rate or Recall), which measures the proportion of true positive instances that are correctly identified by the model, (3) Specificity (True Negative Rate), which measures the proportion of true negative instances that are correctly identified by the model, (4) Matthew's Correlation Coefficient (MCC), which measures the correlation between the observed and predicted binary classification, (5) Precision (Positive Predictive Value), which measures the proportion of true positive instances among all instances predicted as positive by the mode, and (6) G-mean (Geometric Mean), which is the geometric mean of sensitivity and specificity.
